{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "nav_menu": {
      "height": "279px",
      "width": "309px"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Furg - ECD 02 - Machine Learning I - Processo da Análise Preditiva",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atejap05/pos_data_science_furg/blob/main/disciplinas/semana02/Furg_ECD_02_Machine_Learning_I_Processo_da_An%C3%A1lise_Preditiva.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A-ZVxhwNvnz"
      },
      "source": [
        "# Curso de Especialização em Ciência de Dados - FURG\n",
        "## Machine Learning I - Processo da Análise Preditiva\n",
        "### Prof. Marcelo Malheiros\n",
        "\n",
        "Código adaptado de Aurélien Geron (licença Apache-2.0)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG-NGqxbNvn-"
      },
      "source": [
        "# Inicialização\n",
        "\n",
        "Aqui importamos as bibliotecas fundamentais de Python para este _notebook_:\n",
        "\n",
        "- NumPy: suporte a vetores, matrizes e operações de Álgebra Linear\n",
        "- Matplotlib: biblioteca de visualização de dados\n",
        "- Pandas: pacote estatístico e de manipulação de DataFrames\n",
        "- Scikit-Learn: biblioteca com algoritmos de Machine Learning\n",
        "- ScyPy: funções científicas diversas, incluindo estatísticas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbZpp9lINvn_"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWDZUJxlNvoB"
      },
      "source": [
        "# 1. Observar o quadro geral\n",
        "\n",
        "O contexto deste projeto é o desenvolvimento de um sistema, usando Machine Learning, para **prever valores médios de casas em distritos da Califórnia**, considerando uma série de _features_ oriunda do censo destes distritos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei0ch00ENvoC"
      },
      "source": [
        "## 1.1 Contextualizar o problema\n",
        "\n",
        "Para este projeto, a saída do modelo de Machine Learning alimentará outro sistema de ML, juntamente com muitos outros dados. Este segundo sistema determinará se vale a pena investir ou não no setor imobiliário de determinada área."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko-ETA5sNvoD"
      },
      "source": [
        "## 1.2 Qual a saída esperada?\n",
        "\n",
        "Até o momento, os preços de habitação do distrito são estimados manualmente por especialistas, o que é caro e demorado. Além disso, ocorrem erros de previsão.\n",
        "\n",
        "Uma vez que está disponível um conjunto de dados completo sobre tais distritos, incluindo dados econômicos, convém explorar um modelo de previsão baseado em ML. A saída é uma previsão de valores médios de casas para qualquer parte da Califórnia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htFk2U2UNvoF"
      },
      "source": [
        "## 1.3 Enquadrar o problema\n",
        "\n",
        "Esta é uma tarefa típica de **aprendizado supervisionado**, uma vez que já existem exemplos de treinamento rotulados (cada instância vem com o valor esperado, ou seja, o preço médio da habitação do distrito).\n",
        "\n",
        "Também é uma tarefa típica de **regressão**, uma vez deseja-se prever um valor numérico.\n",
        "\n",
        "Em particular, este é um  problema de **regressão múltipla**, pois o sistema usará várias _features_ para fazer uma previsão (população do distrito e renda média, por exemplo).\n",
        "\n",
        "Também é um problema de **regressão univariada**, uma vez que deseja-se prever um único valor para cada distrito.\n",
        "Se houvesse necessidade de prever vários valores por distrito, seria um problema multivariado.\n",
        "\n",
        "Finalmente, não há fluxo contínuo de dados entrando no sistema, de forma que não há necessidade de se ajustar a dados variáveis rapidamente. Como os dados são pequenos o suficiente para caber na memória, é possível usar a estratégia de **aprendizado em lote**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3kCkDC9NvoG"
      },
      "source": [
        "## 1.4 Selecionar uma medida de desempenho\n",
        "\n",
        "Para este problema será utilizada a medida Root Mean Square Error: \n",
        "\n",
        "\\begin{align}\n",
        "\\text{RMSE}(\\textbf{X}, h) = \\sqrt{ \\frac{1}{m} \\sum_{i=1}^{m} \\left( h(\\textbf{x}^{(i)}) - y^{(i)} \\right) ^2 }\n",
        "\\end{align}\n",
        "\n",
        "Para comparação, também será utilizada a medida Mean Absolute Error:\n",
        "\n",
        "\\begin{align}\n",
        "\\text{MAE}(\\textbf{X}, h) = \\frac{1}{m} \\sum_{i=1}^{m} \\left| h(\\textbf{x}^{(i)}) - y^{(i)} \\right|\n",
        "\\end{align}\n",
        "\n",
        "Quanto mais alto o expoente aplicado à diferente, mais o erro se concentra em grandes valores, negligenciando pequenos. É por isso que a RMSE é mais sensível a _outliers_ do que a MAE.\n",
        "\n",
        "Porém, quando _outliers_ são mais raros (como em uma curva em forma de sino), a RMSE executa muito bem e geralmente é a medida preferida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vaeMzbtNvoG"
      },
      "source": [
        "## 1.5 Verifique as premissas\n",
        "\n",
        "Por exemplo, é preciso verificar se os preços previstos estão no formato adequado. No caso, devem ser valores numéricos, em dólares, e não categorias discretas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kZpeFslNvoH"
      },
      "source": [
        "# 2. Obter os dados\n",
        "\n",
        "O conjunto de dados sobre preços de habitação da California é originário do repositório StatLib, e foi obtido de https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
        "\n",
        "Este conjunto de dados é baseado em dados da década de 1990. Mesmo não sendo recente, tem muitas boas qualidades de aprendizado. Também foi adicionado um atributo categórico e removidos alguns _features_ para ilustrar como trabalhar com valores faltantes.\n",
        "\n",
        "Estas estatísticas são por grupo de quarteirões, que é a menor unidade geográfica para a qual o U.S. Census Bureau publica dados de amostra. Por simplicidade, um grupo de quarteirões é chamado de **distrito** nesta análise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t8SciHLNvoI"
      },
      "source": [
        "## 2.1 Examinar os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP45OHdENvoJ"
      },
      "source": [
        "# cada linha representa um distrito\n",
        "housing = pd.read_csv('housing.csv')\n",
        "housing.head(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgToKKzrNvoN"
      },
      "source": [
        "# visão geral\n",
        "housing.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOt2eWdDNvoN"
      },
      "source": [
        "Existem 20.640 instâncias no conjunto de dados, o que significa que é pequeno pelo padrão de Machine Learning, mas é perfeito como exemplo.\n",
        "\n",
        "Observe que o atributo **total_bedrooms** tem apenas 20.433 valores não nulos, o que significa que 207 distritos não possuem este atributo. Isso precisa ser tratado posteriormente.\n",
        "\n",
        "Todos os atributos são numéricos, exceto o campo **ocean_proximity**. Seu tipo é objeto, mas como veio de um arquivo CSV significa que é do tipo _string_. Mais ainda, como há muitas repetições, provavelmente é um atributo categórico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT3oXl4oNvoO"
      },
      "source": [
        "# exame de quantas são as categorias de 'ocean_proximity'\n",
        "housing['ocean_proximity'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUO6vA6gNvoO"
      },
      "source": [
        "# resumo dos atributos numéricos\n",
        "housing.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncTqam06NvoP"
      },
      "source": [
        "As linhas de contagem (**count**), média (**mean**), mínimo (**min**) e máximo (**max**) são autoexplicativas.\n",
        "\n",
        "Valores nulos são ignorados.\n",
        "\n",
        "A linha **std** mostra o desvio padrão, que mede a dispersão dos valores daquela coluna.\n",
        "\n",
        "As linhas **25%**, **50%** e **75%** mostram os percentis correspondentes. Por exemplo, 25% dos distritos têm **housing_median_age** inferior a 18, enquanto 50% são inferiores a 29 e 75% são inferiores a 37. Estes são frequentemente chamados de 25º percentil (ou 1º quartil), mediana e 75º percentil (ou 3º quartil)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRKlfuxQNvoP"
      },
      "source": [
        "# podemos mostrar os histogramas para observar a distribuição de todos os valores\n",
        "housing.hist(bins=50, figsize=(20,15))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di-PMiGdNvoQ"
      },
      "source": [
        "Pontos interessantes:\n",
        "\n",
        "- O atributo **median_income** não está expresso em dólares americanos (USD), mas em milhares de dólares. E ainda foi limitado em 15.\n",
        "\n",
        "- Os atributos **housing_median_age** e **median_house_value** também foram limitados, como é possível observar pelas altas contagens mais à direita nos respectivos histogramas.\n",
        "\n",
        "- O limite de **median_house_value** precisa ser analisado, pois este é o atributo alvo (ou seja, o rótulo). Se for necessário gerar previsões além de 500.000, isso pode ser um problema. Uma solução seria obter novamente estes dados, com mais detalhes. Outra solução seria simplesmente remover este subconunto da base de dados.\n",
        "\n",
        "- Os atributos têm escalas muito diferentes, o que será discutido posteriormente, quando for falado do dimensionamento das _features_.\n",
        "\n",
        "- Muitos histogramas têm uma _longa cauda_ (são **tail heavy**): se estendem muito mais à direita da mediana do que à esquerda. Isso pode tornar mais difícil que algoritmos de ML detectem padrões. A solução é transformar esses atributos para ter distribuições em forma de sino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcISWzYoNvoQ"
      },
      "source": [
        "## 2.2 Criar um conjunto de testes\n",
        "\n",
        "Antes de examinar os dados mais a fundo, é necessário criar o conjunto de teste.\n",
        "\n",
        "Idealmente ele será criado e deixado de lado até mais tarde. Analisar o conjunto de teste pode causar um viés no processo de análise e eventualmente afetar as medidas de desempenho."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Wb7A6sNvoR"
      },
      "source": [
        "### Amostragem aleatória"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkp6-YGQNvoR"
      },
      "source": [
        "# para garantir um resultado idêntico a cada execução deste notebook, \n",
        "# iniciamos a geração de números aleatórios com a mesma semente (seed)\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veg-sIgVNvoR"
      },
      "source": [
        "# exemplo de função simples para separar dados em treino e teste\n",
        "def split_train_test(data, test_ratio):\n",
        "    shuffled_indices = np.random.permutation(len(data))\n",
        "    test_set_size = int(len(data) * test_ratio)\n",
        "    test_indices = shuffled_indices[:test_set_size]\n",
        "    train_indices = shuffled_indices[test_set_size:]\n",
        "    return data.iloc[train_indices], data.iloc[test_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTtgb8o4NvoS"
      },
      "source": [
        "train_set, test_set = split_train_test(housing, 0.2)\n",
        "print('train set:', len(train_set))\n",
        "print('test set: ', len(test_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaXznZ8nNvoS"
      },
      "source": [
        "# a biblioteca scikit-learn já inclui uma função de separação randomizada, muito semelhante à anterior\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
        "print('train set:', len(train_set))\n",
        "print('test set: ', len(test_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAFnf-EVNvoT"
      },
      "source": [
        "Uma abordagem melhor é calcular um _hash_ do identificador de cada instância e colocar essa instância\n",
        "no conjunto de teste se este _hash_ for menor ou igual a 20% do valor máximo da função _hash_.\n",
        "\n",
        "Isso garante que o conjunto de teste permanecerá consistente através de várias execuções, mesmo se o conjunto de dados for atualiado. O novo conjunto de teste conterá 20% das novas instâncias, mas não conterá nenhuma instância que estava anteriormente no conjunto de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pu2x9_ANvoU"
      },
      "source": [
        "import hashlib\n",
        "\n",
        "def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n",
        "    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio\n",
        "\n",
        "def split_train_test_by_id(data, test_ratio, id_column):\n",
        "    ids = data[id_column]\n",
        "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
        "    return data.loc[~in_test_set], data.loc[in_test_set]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A8Qh0e0NvoU"
      },
      "source": [
        "Este conjunto de dados de habitação não tem uma coluna com um identificador. Uma solução simples é usar o índice de linha como este ID.\n",
        "\n",
        "Nesse caso, é preciso se certificar de que os novos dados são anexado ao final do conjunto de dados e que nenhuma linha anterior seja excluída."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKS4mFr0NvoV"
      },
      "source": [
        "# adiciona uma coluna chamada 'index' e faz a separação\n",
        "housing_with_id = housing.reset_index()\n",
        "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 'index')\n",
        "print('train set:', len(train_set))\n",
        "print('test set: ', len(test_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK3av0ZqNvoW"
      },
      "source": [
        "Uma solução mais robusta é usar as _features_ mais estáveis para construir um identificador único.\n",
        "\n",
        "Neste caso, a latitude e longitude de um distrito têm garantia de estabilidade, então podem ser combinadas em um ID numérico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj5sys1kNvoW"
      },
      "source": [
        "# adiciona uma coluna chamada 'id' e faz a separação\n",
        "housing_with_id['id'] = housing['longitude'] * 1000 + housing['latitude']\n",
        "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\n",
        "print('train set:', len(train_set))\n",
        "print('test set: ', len(test_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE-IYbKENvoW"
      },
      "source": [
        "test_set.head(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9qks9sINvoW"
      },
      "source": [
        "### Amostragem estratificada\n",
        "\n",
        "A amostragem aleatória é adequada quando o conjunto de dados é grande (especialmente em relação ao número de atributos), mas se não for, há o risco de introduzir um significativo **viés de amostragem** (_sampling bias_).\n",
        "\n",
        "Para evitar isso podemos fazer uma **amostragem estratificada**, levando em conta a frequência dos atributos presentes nos dados. A ideia é criar conjuntos de treino e de testes que sejam estatisticamente representativos em relação a toda a base de dados.\n",
        "\n",
        "Isso será feito para o atributo **median_income**, ao agrupar faixas de valores em **estratos** (_strata_)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMnY1LUtNvoX"
      },
      "source": [
        "housing['median_income'].hist()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXsqRl7ENvoY"
      },
      "source": [
        "housing['income_cat'] = pd.cut(housing['median_income'],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])\n",
        "housing['income_cat'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYWozsdjNvoZ"
      },
      "source": [
        "housing['income_cat'].hist(bins=5, range=(1,6), align='left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYQGOVpTNvoZ"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# usa amostragem estratificada para separar os dados em treino e teste\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(housing, housing['income_cat']):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]\n",
        "print('train set:', len(strat_train_set))\n",
        "print('test set: ', len(strat_test_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMVDFSKaNvoZ"
      },
      "source": [
        "# percentuais por estrato para 'median_income' no conjunto de treino\n",
        "strat_train_set['income_cat'].value_counts() / len(strat_train_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnKdqOj_Nvoa"
      },
      "source": [
        "# percentuais por estrato para 'median_income' no conjunto de teste\n",
        "strat_test_set['income_cat'].value_counts() / len(strat_test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blFwNIsQNvoa"
      },
      "source": [
        "# percentuais no conjunto de dados completo\n",
        "housing[\"income_cat\"].value_counts() / len(housing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDHA7G2hNvoa"
      },
      "source": [
        "A seguir, uma breve comparação da diferença entre os conjunto original e os conjuntos de teste obtidos com amostragem aleatória e amostragem estratificada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zTFsymVNvob"
      },
      "source": [
        "def income_cat_proportions(data):\n",
        "    return data['income_cat'].value_counts() / len(data)\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
        "\n",
        "compare_props = pd.DataFrame({\n",
        "    'Overall':    income_cat_proportions(housing),\n",
        "    'Random':     income_cat_proportions(test_set),\n",
        "    'Stratified': income_cat_proportions(strat_test_set),\n",
        "}).sort_index()\n",
        "compare_props['Rand. %error']  = 100 * compare_props['Random']     / compare_props['Overall'] - 100\n",
        "compare_props['Strat. %error'] = 100 * compare_props['Stratified'] / compare_props['Overall'] - 100\n",
        "compare_props"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzDFpOX8Nvob"
      },
      "source": [
        "# para finalizar, será removido o atributo auxiliar 'income_cat' dos conjuntos de treino e teste\n",
        "for set_ in (strat_train_set, strat_test_set):\n",
        "    set_.drop('income_cat', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWsRPdblNvoc"
      },
      "source": [
        "# 3. Analisar e visualizar os dados de treino\n",
        "\n",
        "O conjunto de teste será guardado para mais tarde, e uma cópia será feira para explorar melhor os dados de treino."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gfRDALgNvoc"
      },
      "source": [
        "# note que o DataFrame 'housing' a partir de agora é uma cópia dos dados de treino estratificados\n",
        "housing = strat_train_set.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCB6r-Z3Nvoc"
      },
      "source": [
        "## Visualização de dados geográficos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzHBfURXNvod"
      },
      "source": [
        "# uma plotagem usando um gráfico de dispersão permite ver a distribuição geográfica das instâncias\n",
        "housing.plot(kind='scatter', x='longitude', y='latitude')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6vm864FNvod"
      },
      "source": [
        "Esse tipo de exibição tem limitações, principalmente pela sobreposição de informações. Ao se usar transparência, temos uma percepção melhor da densidade dos dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZRIhNTKNvod"
      },
      "source": [
        "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MeAY8VSNvod"
      },
      "source": [
        "Agora uma visualizando incluindo os preços das casas.\n",
        "\n",
        "O raio de cada círculo representa a população do distrito (parâmetro **s**), enquanto a cor representa o preço (parâmetro **c**). A figura uma um mapa de cores predefinido (parâmetro **cmap='jet'**), que varia do azul\n",
        "(preços baixos) para vermelho (preços altos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG31NCZGNvof"
      },
      "source": [
        "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,\n",
        "             s=housing['population']/100, label='population', figsize=(10,7),\n",
        "             c='median_house_value', cmap='jet', colorbar=True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDlCNv06Nvof"
      },
      "source": [
        "O código abaixo lê a imagem **california.png** e a usa como fundo para o gráfico, ilustrando melhor as fronteiras do estado e as regiões de oceano."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqlqB1s9Nvog"
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "\n",
        "# gráfico de dispersão\n",
        "ax = housing.plot(kind='scatter', x='longitude', y='latitude', figsize=(10,7),\n",
        "                  s=housing['population']/100, label='population',\n",
        "                  c='median_house_value', cmap='jet', colorbar=False, alpha=0.4)\n",
        "plt.ylabel('latitude', fontsize=12)\n",
        "plt.xlabel('longitude', fontsize=12)\n",
        "\n",
        "# imagem de fundo\n",
        "california_img = mpimg.imread('california.png')\n",
        "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\n",
        "\n",
        "# barra de valores\n",
        "prices = housing['median_house_value']\n",
        "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
        "cbar = plt.colorbar(ticks=tick_values/prices.max())\n",
        "cbar.ax.set_yticklabels(['$%dk'%(round(v/1000)) for v in tick_values], fontsize=12)\n",
        "cbar.set_label('median house value', fontsize=12)\n",
        "\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe8gIF_-Nvog"
      },
      "source": [
        "## Procurando por correlações entre atributos\n",
        "\n",
        "Como o conjunto de dados não é grande, pode-se calcular facilmente o coeficiente de correlação (também chamado de R de Pearson) entre cada par de atributos usando a função **corr()** de um DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXTvSskgNvog"
      },
      "source": [
        "corr_matrix = housing.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xzMMTM1Nvoh"
      },
      "source": [
        "# quanto cada atributo se correlaciona com o valor de 'median_house_value'\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPmInI6sNvoh"
      },
      "source": [
        "O coeficiente de correlação varia de -1 a 1.\n",
        "\n",
        "Quando está próximo de 1, significa que existe uma forte correlação positiva. Neste exemplo, o atributo de valor das casas tende a aumentar quando a renda média aumenta.\n",
        "\n",
        "Quando o coeficiente está próximo de -1, significa que existe uma forte correlação negativa.\n",
        "\n",
        "Finalmente, um coeficiente de 0 indica que não há correlação linear. Note que não haver correlação linear não exclui a existência de correlações mais complexas entre os dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joVNZUtpNvoh"
      },
      "source": [
        "# a biblioteca Pandas permite exibir graficamente pares de atributos em uma matriz\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "# aqui apenas alguns, mais promissores, são selecionados\n",
        "attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\n",
        "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD08wLKZNvoh"
      },
      "source": [
        "Note que na diagonal da matriz acima são mostrados os histogramas de distribuição de cada atributo.\n",
        "\n",
        "Como o atributo mais promissor para prever o valor mediano das casas é a mediana de renda, abaixo é mostrado um gráfico específico para esta relação.\n",
        "\n",
        "A correlação positiva entre ambos atributos é bem clara, assim como o efeito de limitação do valor máximo das casas. E também são discerníveis linhas horizontais para algumas faixas de valores das casas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tsZBjpxNvoi"
      },
      "source": [
        "housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)\n",
        "plt.axis([0, 16, 0, 550000])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbtoBdh7Nvoi"
      },
      "source": [
        "## Experimentar com combinações entre atributos\n",
        "\n",
        "É conveniente experimentar várias combinações de atributos, que talvez tragam mais clareza sobre os dados.\n",
        "\n",
        "Por exemplo, o número total de cômodos em um distrito não é muito útil quando não se sabe quantas famílias existem. Faz mais sentido ter o número de cômodos por casa.\n",
        "\n",
        "Da mesma forma, o número total de quartos de dormir por distrito por si só não é muito útil: faz sentido compará-lo com o número de cômodos. E a população por domicílio também parece uma combinação de atributos interessante de se olhar.\n",
        "\n",
        "Esse novos atributos serão criados a seguir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhoE3NUpNvoj"
      },
      "source": [
        "housing['rooms_per_household']      = housing['total_rooms'] / housing['households']\n",
        "housing['bedrooms_per_room']        = housing['total_bedrooms'] / housing['total_rooms']\n",
        "housing['population_per_household'] = housing['population'] / housing['households']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHRasG6KNvoj"
      },
      "source": [
        "corr_matrix = housing.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK4LGIRxNvoj"
      },
      "source": [
        "O novo atributo **bedrooms_per_room** está muito mais relacionado com o valor mediano das casas do que o número total de cômodos ou quartos de dormir. Então casas com uma relação quarto / cômodo menor tendem a ser mais caras.\n",
        "\n",
        "O número de quartos por domicílio (**rooms_per_household**) também é mais informativo do que o número total de quartos em um distrito (quanto maiores as casas, mais caras elas são).\n",
        "\n",
        "Essa rodada de exploração não precisa ser completa; o objetivo é obter percepções que ajudem a construir uma boa análise inicial. Este é um processo iterativo: uma vez criado um protótipo de ML, sua saída pode ser analisada para obter mais _insights_, voltando a esta etapa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPNHFGCGNvok"
      },
      "source": [
        "# 4. Preparar os dados para os algoritmos de Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30QuH9n9Nvok"
      },
      "source": [
        "# aqui o dataset é restaurado do ponto onde foi criado o conjunto de treino, separando também os rótulos\n",
        "housing = strat_train_set.drop('median_house_value', axis=1)\n",
        "housing_labels = strat_train_set['median_house_value'].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy0uMIbhNvok"
      },
      "source": [
        "## 4.1 Limpeza dos dados\n",
        "\n",
        "A maioria dos algoritmos de ML não funciona com _features_ ausentes, então é necessário criar algumas funções para cuidar disso.\n",
        "\n",
        "No caso específico, o atributo **total_bedrooms** tem alguns valores faltando."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBXWF5wqNvok"
      },
      "source": [
        "housing[housing.isnull().any(axis=1)].head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4uyw0R4Nvol"
      },
      "source": [
        "São três as opções:\n",
        "\n",
        "1. Remover os distritos correspondentes: \n",
        "\n",
        "       housing.dropna(subset=['total_bedrooms'])\n",
        "\n",
        "2. Remover completaee de todo o atributo:\n",
        "\n",
        "       housing.drop('total_bedrooms', axis=1)\n",
        "\n",
        "3. Definir os valores faltantes para algum novo valor (zero, a média, a mediana, etc.):\n",
        "\n",
        "       median = housing['total_bedrooms'].median()\n",
        "       housing['total_bedrooms'].fillna(median, inplace=True)\n",
        "       \n",
        "Note que na opção 3 este valor de substituição teria que ser posteriormente aplicado para os dados de teste também, além dos novos dados de entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF5el8IPNvol"
      },
      "source": [
        "# a biblioteca scikit-learn permite computar e fazer a substituição automaticamente\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# estratégia de uso da mediana\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "# como a mediana apenas funciona em valores numéricos, é necessária uma base sem o atributo 'ocean_proximity'\n",
        "housing_num = housing.drop('ocean_proximity', axis=1)\n",
        "imputer.fit(housing_num)\n",
        "\n",
        "# os valores computados são guardados neste objeto\n",
        "imputer.statistics_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0g1Cft4Nvol"
      },
      "source": [
        "# por garantia, comparamos com o cálculo manual da mediana para os mesmos atributos\n",
        "housing_num.median().values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F_KSbxmNvom"
      },
      "source": [
        "O novo conjunto de treino, transformado, pode ser criado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL2dVWaDNvom"
      },
      "source": [
        "X = imputer.transform(housing_num)\n",
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing.index)\n",
        "housing_tr.head(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uxA6_SfNvom"
      },
      "source": [
        "## 4.2 Lidando com textos e atributos categóricos\n",
        "\n",
        "O atributo categórico **ocean_proximity** foi deixado de fora porque não é possível calcular sua mediana."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au0MOMFjNvom"
      },
      "source": [
        "housing_cat = housing[['ocean_proximity']]\n",
        "housing_cat.head(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wu3X0soNvom"
      },
      "source": [
        "Como a maioria dos algoritmos de Machine Learning prefere trabalhar com números, podemos converter essa categoria de texto para número. Para isso, podemos usar a classe `OrdinalEncoder`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2ERjrlINvoo"
      },
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
        "\n",
        "# resultado codificado\n",
        "housing_cat_encoded[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMYpbmKYNvoo"
      },
      "source": [
        "# categorias que foram codificadas\n",
        "ordinal_encoder.categories_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3guohbgsNvoo"
      },
      "source": [
        "Um problema com esta representação é que os algoritmos de ML irão assumir que dois valores numericamente próximos\n",
        "são mais semelhantes do que dois valores distantes.\n",
        "\n",
        "Isso pode ser bom em alguns casos (por exemplo, para categorias ordenadas como \"ruim\", \"médio\", \"bom\", \"excelente\"), mas não é o caso da coluna **ocean_proximity**.\n",
        "\n",
        "Para corrigir esse problema, uma solução comum é criar um atributo binário por categoria: um atributo igual a 1 quando a categoria é **<1H OCEAN** (e 0 caso contrário), outro atributo igual a 1 quando a categoria é\n",
        "**INLAND** (e 0 caso contrário), e assim por diante.\n",
        "\n",
        "Isso é chamado de codificação distribuída (_one-hot_), porque apenas um atributo será igual a 1 (quente), enquanto os outros serão 0 (frio)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_tskkUfNvoo"
      },
      "source": [
        "# o codificador OneHotEncoder permite converter valores categóricos em vetores one-hot\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = OneHotEncoder(sparse=False)\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "housing_cat_1hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_jZN06bNvop"
      },
      "source": [
        "# categorias que foram codificadas\n",
        "cat_encoder.categories_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw9pu7OxNvop"
      },
      "source": [
        "## 4.3 Escalonamento de _features_\n",
        "\n",
        "Uma das transformações mais importantes que precisa ser aplicada aos dados é o **escalonamento de _features_**.\n",
        "\n",
        "Com poucas exceções, os algoritmos de aprendizado de máquina não funcionam bem quando os atributos numéricos de entrada têm escalas muito diferentes. Entretanto, redimensionar os valores alvo geralmente não é requerido.\n",
        "\n",
        "Existem duas maneiras comuns de fazer com que todos os atributos tenham a mesma escala: escalonamento min-max\n",
        "e padronização.\n",
        "\n",
        "O **escalonamento min-max** (também chamado de **normalização**) é bastante simples: os valores são deslocados e reescalonados de modo que acabem variando de 0 a 1. Fazemos isso subtraindo o valor mínimo e dividindo pelo máximo menos o mínimo. A classe `MinMaxScaler` provê um transformador para isso.\n",
        "\n",
        "A **padronização** é bem diferente: primeiro, esta subtrai o valor médio e então divide pelo desvio padrão, para que a distribuição resultante tenha média zero e variância unitária. A classe `StandardScaler` fornece um transformador para fazer a padronização."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BupTWyn4Nvoq"
      },
      "source": [
        "## 4.4 Pipelines de transformação\n",
        "\n",
        "Existem muitas etapas de transformação de dados que precisam ser executadas em certa ordem certa.\n",
        "\n",
        "Para isto, o a biblioteca Scikit-Learn fornece a classe `Pipeline` para ajudar com tais sequências de transformações."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSS4-EFpNvoq"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# pipeline para dados numéricos\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer',       SimpleImputer(strategy='median')),\n",
        "    ('std_scaler',    StandardScaler()),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XhmbUxRNvoq"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "# pipeline combinando dados numérico com atributo categórico\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline,    num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "    ])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71oYWFBpNvoq"
      },
      "source": [
        "# estes são os dados de treino já preparados (e com colunas adicionais)\n",
        "housing_prepared.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrXi-l3BNvor"
      },
      "source": [
        "# 5. Selecionar e treinar um modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DfLmPgINvor"
      },
      "source": [
        "## 5.1 Treinar e avaliar um modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0_WJKRkNvor"
      },
      "source": [
        "# aqui vai ser utilizado um modelo simples de Regressão Linear\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI6S0n8qNvor"
      },
      "source": [
        "# pipeline inteiro para algumas instâncias\n",
        "some_data = housing.iloc[:4]\n",
        "some_labels = housing_labels.iloc[:4]\n",
        "some_data_prepared = full_pipeline.transform(some_data)\n",
        "\n",
        "print('previsões:    ', lin_reg.predict(some_data_prepared))\n",
        "print('valores reais:', list(some_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R89RSlgCNvos"
      },
      "source": [
        "# usando a medida de desempenho RMSE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t-EOaB7Nvot"
      },
      "source": [
        "# usando a medida de desempenho MAE\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
        "lin_mae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbrjnSIpNvot"
      },
      "source": [
        "Claramente não é uma boa pontuação: na maioria dos distritos **median_housing_value** varia entre 120.000 e 265.000, então uma previsão típica erro 68.628 ou mesmo 49.439 não é satisfatória.\n",
        "\n",
        "Este é um exemplo de _underfitting_ dos dados de treinamento. Quando isso acontece, pode significar que os recursos não fornecem informações suficientes para fazer boas previsões, ou que o modelo não é poderoso o suficiente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAQ5x7cFNvot"
      },
      "source": [
        "# aqui um modelo de Árvore de Decisão\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi4-Gzq5Nvot"
      },
      "source": [
        "# usando a medida de desempenho RMSE\n",
        "housing_predictions = tree_reg.predict(housing_prepared)\n",
        "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "tree_rmse = np.sqrt(tree_mse)\n",
        "tree_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASepyaO1Nvou"
      },
      "source": [
        "Erro zero indica muito provavelmente que o modelo tenha superestimado os dados. Ou seja, um caso de _overfitting_.\n",
        "\n",
        "Note que o correto é **somente usar o conjunto de teste quando haja confiança em um modelo já treinado**.\n",
        "\n",
        "Então é necessário usar parte do conjunto de treinamento para treinar e parte para validação do modelo, o que é chamado de validação cruzada (_cross validation_)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UywCSa6hNvou"
      },
      "source": [
        "## 5.2 Avaliar usando validação cruzada\n",
        "\n",
        "Esta validação pode ser manualmente quebrando o conjunto de treino em partes menores e criando modelos separados, que são avaliados também separadamente.\n",
        "\n",
        "Este é um processo muito comum de Machine Learning, e pode ser efetuado automaticamente usando `cross_val_score`.\n",
        "\n",
        "O código a seguir divide aleatoriamente o conjunto de treinamento em 10 subconjuntos distintos chamados dobras (_folds_) e em seguida, treina e avalia o modelo de árvore de decisão 10 vezes.\n",
        "\n",
        "O processo escolhe uma dobra diferente para avaliação em cada vez, utilizando as outras 9 dobras para treinamento. O resultado é um vetor contendo as 10 pontuações de avaliação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SUt3fSgNvou"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbTqqjRXNvov"
      },
      "source": [
        "# validação cruzada para o modelo de Árvore de Decisão criado anteriomente\n",
        "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, \n",
        "                         scoring='neg_mean_squared_error', cv=10)\n",
        "scores = np.sqrt(-scores)\n",
        "print(\"Scores:\", scores)\n",
        "print(\"Mean:  \", scores.mean())\n",
        "print(\"Std:   \", scores.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMxIzcTTNvow"
      },
      "source": [
        "A validação cruzada permite obter não apenas uma estimativa do desempenho do modelo, mas também uma medida\n",
        "de quão precisa essa estimativa é (ou seja, seu desvio padrão).\n",
        "\n",
        "Essa informação não seria possível sem uma sistemática de validação. Mas a validação cruzada vem com o custo de treinar o modelo várias vezes, o que pode ser um limitante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v1ZgW43Nvoy"
      },
      "source": [
        "# validação cruzada para o modelo de Regressão Linear criado anteriomente\n",
        "scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
        "                         scoring='neg_mean_squared_error', cv=10)\n",
        "scores = np.sqrt(-scores)\n",
        "print(\"Scores:\", scores)\n",
        "print(\"Mean:  \", scores.mean())\n",
        "print(\"Std:   \", scores.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4Rm-9IPNvoy"
      },
      "source": [
        "De fato o modelo da Árvore de Decisão está fazendo _overfitting_, tanto que tem um desempenho pior\n",
        "do que o modelo de Regressão Linear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO0FMEvLNvoy"
      },
      "source": [
        "### Modelo Ensemble\n",
        "\n",
        "Aqui será testado mais um modelo, `RandomForestRegressor`.\n",
        "\n",
        "Como será visto posteriormente, Florestas Aleatórias trabalham treinando muitas Árvores de Decisão em subconjuntos aleatórios de _features_ e, em seguida, calculando a média de suas previsões.\n",
        "\n",
        "A construção de um modelo em cima de muitos outros modelos é chamadas de Ensemble Learning (significando aprendizado por comitê ou agregação)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "venXDo0WNvoy"
      },
      "source": [
        "# aqui um modelo de Floresta Aleatória -- demora um pouco mais que os anteriores\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "forest_reg.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I78XW4OBNvoz"
      },
      "source": [
        "# usando a medida de desempenho RMSE\n",
        "housing_predictions = forest_reg.predict(housing_prepared)\n",
        "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)\n",
        "forest_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16mb0as1Nvo0"
      },
      "source": [
        "# validação cruzada para o modelo de Floresta Aleatória criado anteriomente -- demora um pouco mais\n",
        "scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
        "                         scoring='neg_mean_squared_error', cv=10)\n",
        "scores = np.sqrt(-scores)\n",
        "print(\"Scores:\", scores)\n",
        "print(\"Mean:  \", scores.mean())\n",
        "print(\"Std:   \", scores.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbEIKqQ0Nvo0"
      },
      "source": [
        "Note que apesar do erro da Floresta Aleatória para todos os dados de treino ser menor, pela validação cruzada é possível perceber que o modelo ainda não é bom o suficiente.\n",
        "\n",
        "Isso indica que o primeiro resultado também teve _overfitting_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qSdKLQnNvo0"
      },
      "source": [
        "### DICA:\n",
        "\n",
        "Cada modelo que foi treinado pode ser salvo e recuperado, para que se possa ser facilmente reutilizado e comparado a outros. Um recurso útil para isso é o módulo `joblib`. Por exemplo\n",
        "\n",
        "    from sklearn.externals import joblib\n",
        "    \n",
        "    joblib.dump(my_model, 'my_model.pkl')\n",
        "    # e depois...\n",
        "    my_model_loaded = joblib.load('my_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuqdOWEfNvo0"
      },
      "source": [
        "# 6. Ajustar o modelo\n",
        "\n",
        "O processo de escolha de hiperparâmetros pode ser automatizado, por exemplo usando a classe `GridSearchCV`.\n",
        "\n",
        "## Grid Search\n",
        "\n",
        "Basta indicar quais hiperparâmetros deverão ser experimentado e quais valores serão testados. Esta classe irá  avaliar todas as combinações possíveis de valores de hiperparâmetros, usando validação cruzada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a9SEZF-Nvo0"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# especificação de hiperparâmetros e suas variações\n",
        "param_grid = [\n",
        "    # vai tentar 12 (3×4) combinações de hiperparâmetros\n",
        "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        "    # então vai tentar 6 (2×3) combinações com 'bootstrap=False'\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "  ]\n",
        "\n",
        "# modelo de Floresta Aleatória\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# treinamento usando 5 dobras, o que dá um total de (12+6)*5=90 rodadas de treinamento\n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "grid_search.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp4R_k_WNvo1"
      },
      "source": [
        "Aqui está a melhor combinação de hiperparâmetros encontrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq6MhaNkNvo2"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkl-aPq0Nvo2"
      },
      "source": [
        "grid_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdXTu7ZmNvo2"
      },
      "source": [
        "Aqui estão todas as pontuações das combinações hiperparâmetros testadas durante a busca em _grid_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgoLxO7BNvo3"
      },
      "source": [
        "cvres = grid_search.cv_results_\n",
        "for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n",
        "    print(np.sqrt(-mean_score), params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGjenS8VNvo3"
      },
      "source": [
        "## Randomized Search\n",
        "\n",
        "A abordagem de pesquisa em grade é boa quando se está explorando relativamente poucas combinações, mas quando o espaço de pesquisa do hiperparâmetro é grande, é frequentemente preferível usar `RandomizedSearchCV`.\n",
        "\n",
        "Esta classe pode ser usada da mesma forma que a classe `GridSearchCV`, mas em vez de tentar todas as combinações possíveis, esta avalia apenas um determinado número de combinações aleatórias, selecionando um valor para cada hiperparâmetro em cada iteração.\n",
        "\n",
        "Esta abordagem tem dois benefícios principais:\n",
        "\n",
        "- Se a pesquisa aleatória for executada por 1.000 iterações, por exemplo, esta abordagem irá explorar 1.000 valores diferentes para cada hiperparâmetro (em vez de apenas alguns valores por hiperparâmetro, com na pesquisa em grade).\n",
        "\n",
        "- É possível ter mais controle sobre a quantidade de processamento a ser gasta com a pesquisa de hiperparâmetros, simplesmente definindo o número de iterações."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t9NHezZNvo3"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "param_distribs = {\n",
        "        'n_estimators': randint(low=1, high=200),\n",
        "        'max_features': randint(low=1, high=8),\n",
        "    }\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
        "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "rnd_search.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "supzKnA-Nvo3"
      },
      "source": [
        "cvres = rnd_search.cv_results_\n",
        "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IhIRqE4Nvo3"
      },
      "source": [
        "## Analisar os melhores modelos e seus erros\n",
        "\n",
        "Bons _insights_ sobre o problema podem ser obtidos ao se inspecionar os melhores modelos.\n",
        "\n",
        "Por exemplo, o `RandomForestRegressor` pode indicar a importância relativa de cada atributo para fazer previsões precisas, como mostrado abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCV9XZ27Nvo4"
      },
      "source": [
        "# usando os resultados da busca em grade\n",
        "feature_importances = grid_search.best_estimator_.feature_importances_\n",
        "cat_encoder = full_pipeline.named_transformers_['cat']\n",
        "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
        "attributes = num_attribs + cat_one_hot_attribs\n",
        "sorted(zip(feature_importances, attributes), reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OzaL8kTNvo4"
      },
      "source": [
        "Com essas informações é possível eliminar alguns dos recursos menos úteis (por exemplo, aparentemente, apenas uma categoria **ocean_proximity** é realmente útil, então pode-se tentar abandonar as demais).\n",
        "\n",
        "Também deve-se olhar para os erros específicos que o sistema comete e então tentar entender entender por que isso aconteceu e o que poderia ser feito para resolver o problema (adicionar recursos extras, livrar-se dos dados não informativos, limpar outliers, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6zEvNS1Nvo4"
      },
      "source": [
        "# 7. Avaliar o modelo final\n",
        "\n",
        "Nesta etapa finalmente são usados o dados de teste. Atente para o uso de `full_pipeline.transform()` para os dados de teste, ao invés de `full_pipeline.fit_transform()`, usado apenas para os dados de treino."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNbqO2ngNvo4"
      },
      "source": [
        "final_model = grid_search.best_estimator_\n",
        "\n",
        "X_test = strat_test_set.drop('median_house_value', axis=1)\n",
        "y_test = strat_test_set['median_house_value'].copy()\n",
        "\n",
        "X_test_prepared = full_pipeline.transform(X_test)\n",
        "final_predictions = final_model.predict(X_test_prepared)\n",
        "\n",
        "final_mse = mean_squared_error(y_test, final_predictions)\n",
        "final_rmse = np.sqrt(final_mse)\n",
        "final_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXM8vz8_Nvo5"
      },
      "source": [
        "Podemos calcular um intervalo de confiança de 95% para o teste RMSE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVXYOpCiNvo5"
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "confidence = 0.95\n",
        "squared_errors = (final_predictions - y_test) ** 2\n",
        "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
        "                         loc=squared_errors.mean(),\n",
        "                         scale=stats.sem(squared_errors)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}