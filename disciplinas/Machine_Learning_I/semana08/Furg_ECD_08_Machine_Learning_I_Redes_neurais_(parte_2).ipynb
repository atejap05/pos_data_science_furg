{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "nav_menu": {
      "height": "264px",
      "width": "369px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Furg - ECD 08 - Machine Learning I - Redes neurais (parte 2)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atejap05/pos_data_science_furg/blob/main/disciplinas/Machine_Learning_I/semana08/Furg_ECD_08_Machine_Learning_I_Redes_neurais_(parte_2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIp59L1kgxJY"
      },
      "source": [
        "# Curso de Especialização em Ciência de Dados - FURG\n",
        "## Machine Learning I - Redes Neurais (parte 2)\n",
        "### Prof. Marcelo Malheiros\n",
        "\n",
        "Código adaptado de Aurélien Geron (licença Apache-2.0)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0odDHfhoqULb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl3B1s_ugxJj"
      },
      "source": [
        "# Inicialização\n",
        "\n",
        "Aqui importamos as bibliotecas fundamentais de Python para este _notebook_:\n",
        "\n",
        "- NumPy: suporte a vetores, matrizes e operações de Álgebra Linear\n",
        "- Matplotlib: biblioteca de visualização de dados\n",
        "- Pandas: pacote estatístico e de manipulação de DataFrames\n",
        "- Scikit-Learn: biblioteca com algoritmos de Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taIrYqxBgxJl"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elv3AUmSgxJm"
      },
      "source": [
        "Este _notebook_, em particular, utiliza a biblioteca Keras para definir e treinar redes neurais. Aqui utilizamos a versão **integrada** de Keras, que já vem como parte da biblioteca mais geral TensorFlow.\n",
        "\n",
        "Em geral é mais fácil usar a versão integrada de Keras, pois esta está pronta para usar a Tensorflow, sem risco de incompatibilidade.\n",
        "\n",
        "Ambas já fazem parte do ambiente Colaboratory.\n",
        "\n",
        "**Atenção:** para quem utiliza o ambiente Jupyter, é preciso primeiro instalar o pacote `tensorflow`. Na linha de comando isso pode ser feito assim:\n",
        "\n",
        "    conda install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l8ePHEIgxJo"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print('tensorflow:      versão', tf.__version__)\n",
        "print('keras integrada: versão', keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMGYQoWvgxJp"
      },
      "source": [
        "Este _notebook_ também utiliza a biblioteca `pydot` e a ferramenta Graphviz para visualizar as redes neurais. \n",
        "\n",
        "Ambos já fazem parte do ambiente Colaboratory.\n",
        "\n",
        "**Atenção:** para quem utiliza o ambiente Jupyter, é preciso primeiro instalar os pacotes `pydot` e `graphviz`. Na linha de comando isso pode ser feito assim:\n",
        "\n",
        "    conda install pydot graphviz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlHidnAsgxJq"
      },
      "source": [
        "import pydot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48sYEY-_gxJs"
      },
      "source": [
        "# Construindo um regressor MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohKx26i-gxJs"
      },
      "source": [
        "Aqui vamos usar o conjunto clássico de dados sobre habitação na Califórnia para implementar um regressor usando redes neurais.\n",
        "\n",
        "Mais especificadamente, vamos usar a versão deste _dataset_ provida pela biblioteca Sciki-Learn através da função `fetch_california_housing()`. São 20640 instâncias, cada uma com 8 _features_ e mais um rótulo. Este conjunto contém apenas valores numéricos (ou seja, sem dados categóricos) e também não há valores faltantes.\n",
        "\n",
        "Os passos iniciais de carregamento, separação em conjuntos (treino, validação e teste) e um rápido preprocessamento (apenas escalonando os valores para a mesma faixa numérica) são mostrados a seguir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1_9ghmNgxJt"
      },
      "source": [
        "# leitura do dataset\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# separação inicial em conjuntos de treino completo (75%) e de teste (25%)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target,\n",
        "                                                              random_state=42, train_size=0.75)\n",
        "\n",
        "# separação posterior do conjuntos de treino completo em dados de treino (75%) e de validação (25%)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full,\n",
        "                                                      random_state=42, train_size=0.75)\n",
        "\n",
        "# preprocessamento\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train) # a escala é definida e então a transformação é feita\n",
        "X_valid = scaler.transform(X_valid)     # apenas a transformação é feita (com a escala previamente definida)\n",
        "X_test  = scaler.transform(X_test)      # apenas a transformação é feita (com a escala previamente definida)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC2S1hxNgxJu"
      },
      "source": [
        "print('treino:   ', X_train.shape,  ' rótulos de treino:   ', y_train.shape)\n",
        "print('validação:', X_valid.shape, '  rótulos de validação:', y_valid.shape)\n",
        "print('teste:    ', X_test.shape,  '  rótulos de teste:    ', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz9kNJT-gxJx"
      },
      "source": [
        "# Criando a rede neural\n",
        "\n",
        "Aqui vamos criar uma rede neural de regressão, usando um modelo (ou arquitetura) do tipo sequencial. O modelo sequencial corresponde ao tipo mais simples de rede neural, onde uma sequência de camadas de neurônios é empilhada uma em cima da outra.\n",
        "\n",
        "É importante também observar que o conjunto de dados `housing` tem **muito ruído**. Então é preferível usarmos apenas uma camada oculta com poucos neurônios, justamente para evitar overfitting. Em outras palavras, optamos por uma arquitetura de rede neural bem simples.\n",
        "\n",
        "- A criação começa com a chamada a `Sequential`, que define o tipo do modelo:\n",
        "\n",
        "        model = keras.models.Sequential()\n",
        "\n",
        "- Então uma camada do tipo `Dense` é adicionada, que é do tipo totalmente conectada com a camada anterior. Esta conta com 30 neurônios e função de ativação ReLU. Como esta é a primeira camada, é preciso também definir o formato da entrada com `input_shape` (neste caso, com 8 entradas):\n",
        "\n",
        "        model.add(keras.layers.Dense(30, activation='relu', input_shape=[8]))\n",
        "        \n",
        "- Daí podemos adicionar uma camada de saída, com tipo também `Dense`, mas sem função de ativação (portanto com o parâmetro `None`). A ideia é que o único neurônio da camada não tenha nenhum tipo de modificação, e justamente produza a previsão numérica deste regressor:\n",
        "        \n",
        "        model.add(keras.layers.Dense(1, activation=None))\n",
        "\n",
        "Ainda que se possa criar uma rede neural com as diversas chamadas a `model.add(...)`, é mais conveniente criar o modelo passando uma lista de camadas, como mostrado a seguir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UftO6WTzgxJx"
      },
      "source": [
        "# comando para 'zerar' a biblioteca Keras\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# definição de sementes aleatórias\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHlsscmTgxJy"
      },
      "source": [
        "# especificação do modelo\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation='relu', input_shape=[8]),\n",
        "    keras.layers.Dense(1, activation=None)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCDI1LA3gxJy"
      },
      "source": [
        "# resumo legível da arquitetura deste modelo\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu_xncgGgxJz"
      },
      "source": [
        "Note que a primeira camada densa tem pesos de conexão de 8 × 30, além de mais 30 termos de _bias_, chegando a um total de 270 parâmetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO-mAS5IgxJz"
      },
      "source": [
        "## Arquitetura da rede neural\n",
        "\n",
        "Podemos gerar uma figura da arquitetura deste modelo usando a função `keras.utils.plot_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8GPDW6VgxJz"
      },
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jd0HwOogxJ0"
      },
      "source": [
        "## Acesso às camadas\n",
        "\n",
        "A biblioteca Keras permite acessar cada camada criada, usando índices de acesso tal como em uma lista de Python.\n",
        "\n",
        "Permite também ver atributos de cada camada e também inspecionar os pesos de todas as conexões desta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMWSf-26gxJ1"
      },
      "source": [
        "# acesso a cada uma das camadas\n",
        "model.layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_SgaBklgxJ1"
      },
      "source": [
        "# primeira camada e respectivo nome\n",
        "hidden1 = model.layers[0]\n",
        "hidden1.name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERXiRkZ0gxJ2"
      },
      "source": [
        "Observe que uma camada `Dense` começa com pesos aleatórios para suas conexões. Os vieses são sempre inicializados com zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEu0GSVVgxJ3"
      },
      "source": [
        "# obtém pesos e vieses da primeira camada\n",
        "weights, biases = hidden1.get_weights()\n",
        "print('weights:', weights.shape)\n",
        "print('biases: ', biases.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZLQ2M5YgxJ3"
      },
      "source": [
        "weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ovaSdPXgxJ3"
      },
      "source": [
        "biases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s978syx8gxJ4"
      },
      "source": [
        "## Compilando a rede neural\n",
        "\n",
        "Depois que um modelo é criado, é preciso chamar o método `compile()`, especificando a **função de perda**. Como é uma tarefa de regressão, deseja-se reduzir o erro quadrado médio, então a função é `mean_squared_error`.\n",
        "\n",
        "Como é típico para redes neurais, o treinamento será feito usando **backpropagation**. Nesse caso, continuamos usando `sgd` como **otimizador**, ou seja, o algoritmo de descida do gradiente estocástico.\n",
        "\n",
        "Como o erro médio quadrado é justamente a nossa medida do quão bom será o treino, validação e teste, não é necessário definir nenhuma outra medida de desempenho."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vShgQns6gxJ5"
      },
      "source": [
        "# especificação da função de perda e do algoritmo de otimização\n",
        "model.compile(loss='mean_squared_error', optimizer='sgd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_OPAwf6gxJ5"
      },
      "source": [
        "# Treinando e avaliando a rede neural\n",
        "\n",
        "Para treinar o modelo basta chamar o método `fit()`. \n",
        "\n",
        "Três parâmetros são obrigatórios: as _features_ de treinamento, os rótulos de treinamento e o número de épocas.\n",
        "\n",
        "Cada **época** (_epoch_) corresponde a uma etapa de atualização da rede neural.\n",
        "\n",
        "Opcionalemente é passado também um conjunto de validação. A biblioteca Keras medirá a função de perda (o erro, neste caso) ao final de cada época, o que é muito útil para ver como o modelo realmente funciona: se o desempenho no conjunto de treinamento é muito melhor do que no conjunto de validação, provavelmente está ocorrendo _overfitting_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0zDnD26gxJ6"
      },
      "source": [
        "# treinamento (esta chamada pode demorar um pouco)\n",
        "%time history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M286Ofs0gxJ6"
      },
      "source": [
        "Agora, após o treinamento, vamos examinar os pesos e vieses ajustados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJYTxmWQgxJ7"
      },
      "source": [
        "# obtém pesos e vieses da primeira camada\n",
        "weights, biases = hidden1.get_weights()\n",
        "weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu25LE-hgxJ7"
      },
      "source": [
        "biases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W23wF0d_gxJ7"
      },
      "source": [
        "## Avaliação final do modelo e geração de previsões"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkuCX7v9gxJ8"
      },
      "source": [
        "# avaliação com conjunto de teste\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8YfgQ0vgxJ8"
      },
      "source": [
        "# previsões computadas para três instâncias de teste\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)\n",
        "print('previsões: ', y_pred.ravel().round(3))\n",
        "print('rótulos:   ', y_test[:3].ravel().round(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qam88mQXgxJ9"
      },
      "source": [
        "## Visualização da evolução das métricas ao longo do treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrRlYx1PgxJ9"
      },
      "source": [
        "# os dados do treinamento estão disponíveis no histórico retornado\n",
        "print('parâmetros:', history.params)\n",
        "print('métricas:  ', list(history.history.keys()))\n",
        "print('épocas:    ', history.epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA-NRKJHgxJ-"
      },
      "source": [
        "# exibição das funções de perda de treino e de validação, para cada época (eixo horizontal)\n",
        "pd.DataFrame(history.history).plot(figsize=(10, 4))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Vk9Qi0gxJ-"
      },
      "source": [
        "# Gravação e recuperação de um modelo treinado\n",
        "\n",
        "Uma vez que o modelo tenha sido treinado e testado, ele está pronto para o uso. Isso permite inclusive que ele possa ser salvo em um arquivo e utilizado posteriormente, sem necessidade de repetir todo o treinamento.\n",
        "\n",
        "A gravação do modelo é feita pela função `.save()` do modelo, que armazena todas informações do mesmo em um único arquivo.\n",
        "\n",
        "Em qualquer momento posterior, tipicamente em programas diferentes, este arquivo pode ser carregado para a memória com o comando `keras.models.load_model()`, permitindo que o modelo seja reconstruído exatamente como antes.\n",
        "\n",
        "**Dica:** a biblioteca TensorFlow pode gerar uma advertência caso muitas chamadas sejam feitas para a função `.predict()`. A ideia da biblioteca é alertar que muitas \"previsões pequenas\" possam ser ineficientes, mas isto aqui não é problema. Para que não sejam geradas essas advertências, basta converter as instâncias a serem previstas usando `tf.constant()` antes de chamar a predição, como é exemplificado abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjX1LkOpgxJ_"
      },
      "source": [
        "# algumas previsões para o modelo já treinado\n",
        "y_pred = model.predict(tf.constant(X_test[:10]))\n",
        "print('previsões: ', y_pred.ravel().round(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_-dRBDlgxJ_"
      },
      "source": [
        "# '.h5' é a extensão do formato Hierarchical Data Format versão 5 (HDF5)\n",
        "model.save('housing_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOs_vwS2gxJ_"
      },
      "source": [
        "# posteriormente o modelo pode ser reconstruído (aqui em outro objeto 'model2')\n",
        "model2 = keras.models.load_model('housing_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arno3-5cgxJ_"
      },
      "source": [
        "# mesmas previsões para o novo modelo recuperado do arquivo\n",
        "y_pred2 = model2.predict(tf.constant(X_test[:10]))\n",
        "print('previsões: ', y_pred2.ravel().round(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD-hLc5TgxKA"
      },
      "source": [
        "# Construindo diferentes arquiteturas com Keras\n",
        "\n",
        "Existem três maneiras de criar modelos de redes neurais com a biblioteca Keras:\n",
        "\n",
        "- A forma mais direta é com a **Sequential API**, onde a arquitetura é justamente uma sequência de camadas. A principal limitação é ter uma única camada de entrada e uma única camada de saída.\n",
        "\n",
        "- Keras também provê a **Functional API**, que é uma API de programação que permite especificar modelos mais complexos e com arquiteturas arbitrárias.\n",
        "\n",
        "- Ainda é possível programar modelos \"do zero\" com a **Subclassing API**, usando o conceito de subclasses e implementando manualmente os detalhes de uma arquitetura. Assim é possível criar modelos dinâmicos, que podem mudar ao longo do tempo.\n",
        "\n",
        "A seguir vamos apenas ilustrar a especificação de algumas arquiteturas usando a API Funcional, sem entrar em maiores detalhes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4hrpD_jgxKA"
      },
      "source": [
        "## Exemplo 1 - Wide and Deep Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss1G6Gp9gxKA"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "input_A = keras.layers.Input(shape=[8])\n",
        "hidden1 = keras.layers.Dense(30, activation='relu')(input_A)\n",
        "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
        "concat  = keras.layers.concatenate([input_A, hidden2])\n",
        "output  = keras.layers.Dense(1)(concat)\n",
        "example = keras.models.Model(inputs=[input_A], outputs=[output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG7UtSnZgxKB"
      },
      "source": [
        "example.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzBeHMDJgxKB"
      },
      "source": [
        "keras.utils.plot_model(example, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S22EDqIgxKC"
      },
      "source": [
        "## Exemplo 2 - Múltiplas entradas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTPmDc9xgxKC"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "input_A = keras.layers.Input(shape=[6], name='wide_input')\n",
        "input_B = keras.layers.Input(shape=[5], name='deep_input')\n",
        "hidden1 = keras.layers.Dense(30, activation='relu')(input_A)\n",
        "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
        "concat  = keras.layers.concatenate([input_B, hidden2])\n",
        "output  = keras.layers.Dense(1, name='output')(concat)\n",
        "example = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYppm92egxKD"
      },
      "source": [
        "keras.utils.plot_model(example, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgXSo6H3gxKE"
      },
      "source": [
        "## Exemplo 3 - Múltiplas entradas e múltiplas saídas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6hzCONigxKE"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "input_A = keras.layers.Input(shape=[6], name='wide_input')\n",
        "input_B = keras.layers.Input(shape=[5], name='deep_input')\n",
        "hidden1 = keras.layers.Dense(30, activation='relu')(input_A)\n",
        "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
        "concat  = keras.layers.concatenate([input_B, hidden2])\n",
        "output1 = keras.layers.Dense(1, name='main_output')(concat)\n",
        "output2 = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
        "example = keras.models.Model(inputs=[input_A, input_B], outputs=[output1, output2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wH503RAgxKE"
      },
      "source": [
        "keras.utils.plot_model(example, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkQy1hDEgxKF"
      },
      "source": [
        "# Visualização usando TensorBoard\n",
        "\n",
        "TensorBoard é uma excelente ferramenta de visualização interativa, muito útil para analisar as curvas de aprendizado do treinamento.\n",
        "\n",
        "Esta ferramenta também permite comparar curvas de aprendizado entre várias execuções, analisar estatísticas de treinamento, visualizar imagens geradas pelo modelo e inspecionar detalhes internos do modelo, por exemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeONBnODgxKF"
      },
      "source": [
        "# comando para 'zerar' a biblioteca Keras\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# definição de sementes aleatórias\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# especificação do modelo\n",
        "model1 = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation='relu', input_shape=[8]),\n",
        "    keras.layers.Dense(1, activation=None)\n",
        "])\n",
        "\n",
        "# especificação da função de perda e do algoritmo de otimização\n",
        "model1.compile(loss='mean_squared_error', optimizer='sgd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK_wwGSNgxKG"
      },
      "source": [
        "TensorBoard faz parte da biblioteca TensorFlow, portanto não é necessária nenhuma instalação adicional.\n",
        "\n",
        "É necessario apenas modificar o programa para que ele armazene os dados a serem visualizados em arquivos binários de _log_. A melhor estratégia é criar um diretório principal, e dentro deste um subdiretório para cada execução do treinamento. Assim é possível comparar visualmente diferentes rodadas de treino.\n",
        "\n",
        "No código abaixo, na chamada de treinamento adicionamos o parâmetro `callbacks=[tensorboard]`, justamente para que o TensorBoard tenha acesso aos dados. Adicionalmente, usamos o parâmetro `verbose=0` para não exibir as barras de progresso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d1qcTs7gxKG"
      },
      "source": [
        "# define o diretório principal para os logs\n",
        "\n",
        "import os\n",
        "log_dir = os.path.join(os.curdir, 'logs')\n",
        "\n",
        "# define o subdiretório de treinamento com base na data e hora atuais\n",
        "\n",
        "import time\n",
        "run_dir1 = os.path.join(log_dir, time.strftime('run_%Y_%m_%d-%H_%M_%S'))\n",
        "\n",
        "# vincula o tensorboard ao subdiretório de treinamento e depois faz o treino\n",
        "\n",
        "tensorboard1 = keras.callbacks.TensorBoard(run_dir1)\n",
        "%time history1 = model1.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), \\\n",
        "                            callbacks=[tensorboard1], verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT9RtPj5gxKG"
      },
      "source": [
        "Tanto o ambiente Colaboratory como a ferramenta Jupyter possuem suporte para integrar o TensorBoard dentro de um _notebook_.\n",
        "\n",
        "É justamente isso que fazem os comandos a seguir, lendo os _logs_ a partir do diretório principal `logs`. Nesta primeira execução, só há um subdiretório."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoMhWIDEgxKG"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./logs --port=6006"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYt7JlUGgxKH"
      },
      "source": [
        "Agora vamos criar um segundo modelo, mudando apenas o hiperparâmetro de taxa de aprendizagem (_learning rate_) do algoritmo SGD.\n",
        "\n",
        "Quando treinarmos este segundo modelo, um novo subdiretório será criado dentro de `logs`, e então podemos atualizar o painel do TensorBoard acima para visualizar e comparar as duas curvas de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7YKHiL0gxKH"
      },
      "source": [
        "model2 = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation='relu', input_shape=[8]),\n",
        "    keras.layers.Dense(1, activation=None)\n",
        "])\n",
        "\n",
        "model2.compile(loss='mean_squared_error', optimizer=keras.optimizers.SGD(lr=0.05))\n",
        "\n",
        "run_dir2 = os.path.join(log_dir, time.strftime('run_%Y_%m_%d-%H_%M_%S'))\n",
        "\n",
        "tensorboard2 = keras.callbacks.TensorBoard(run_dir2)\n",
        "%time history2 = model2.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), \\\n",
        "                            callbacks=[tensorboard2], verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d25Asl8RgxKI"
      },
      "source": [
        "# Ajuste de hiperparâmetros de redes neurais\n",
        "\n",
        "Para um modelo construído com a **Sequential API**, os principais hiperparâmetros são:\n",
        "\n",
        "- O número de camadas ocultas.\n",
        "\n",
        "- O número de neurônios em cada camada oculta.\n",
        "\n",
        "- A taxa de aprendizado (_learning rate_), que é o hiperparâmetro mais importante. O padrão para o SGD é 0.01.\n",
        "\n",
        "- O otimizador utilizado, uma vez que há outras opções além do SGD.\n",
        "\n",
        "- O tamanho do lote de treinamento (_batch size_), usualmente 32.\n",
        "\n",
        "- A função de ativação (_activation function_). Tipicamente a função ReLU é usada para neurônios ocultos. Funções específicas para cada tarefa (regressão ou classificação) são usadas na camada de saída, assim como uma função de perda adequada.\n",
        "\n",
        "- O número de épocas de treinamento.\n",
        "\n",
        "O código abaixo permite experimentar explicitamente com alguns destes parâmetros, ainda usando o _dataset_ `housing`. Cabe destacar o parâmetro `callbacks = [keras.callbacks.EarlyStopping(patience=10)]`, que diz que o treinamento pode parar mais cedo caso não haja redução da função de perda por 10 épocas seguidas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vro_6uG0gxKI"
      },
      "source": [
        "neurons        = 30     # 10 30 50\n",
        "activation     = 'relu' # 'relu' 'sigmoid' 'tanh'\n",
        "learning_rate  = 0.01   # 0.005 0.01 0.05\n",
        "batch_size     = 32     # 16 32 64\n",
        "epochs         = 20     # 10 20 50 100\n",
        "callbacks      = []     # [keras.callbacks.EarlyStopping(patience=10)]\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(neurons, activation=activation, input_shape=[8]),\n",
        "#   keras.layers.Dense(neurons, activation=activation),\n",
        "    keras.layers.Dense(1, activation=None)\n",
        "])\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.SGD(lr=learning_rate))\n",
        "\n",
        "%time history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), \\\n",
        "                          batch_size=batch_size, epochs=epochs, callbacks=callbacks, verbose=0)\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(10, 4))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "print('erro de treino:   ', history.history['loss'][-1])\n",
        "print('erro de validação:', history.history['val_loss'][-1])\n",
        "print('erro de teste:    ', model.evaluate(X_test, y_test, verbose=0))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}